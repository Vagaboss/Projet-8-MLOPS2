# Etape 1 

üóÇÔ∏è Structure du projet

Le projet est organis√© de mani√®re modulaire afin de s√©parer les diff√©rentes logiques : entra√Ænement, inf√©rence, API, monitoring et visualisation. Voici une description des principaux dossiers et fichiers :

üìÅ Dossiers

- api/ : Contient le code li√© √† l‚ÄôAPI Gradio (par ex. logger.py, fonctions d'inf√©rence, etc.).

- datasets/ : Donn√©es utilis√©es pour l‚Äôentra√Ænement, la validation et les tests (train.csv, - test_final.csv, etc.).

- logs/ : Fichiers de logs g√©n√©r√©s automatiquement lors des pr√©dictions (log_prediction()).

- models/ : Mod√®les sauvegard√©s (best_model.pkl, .onnx, quantifi√©, etc.), seuils et fichiers de features.

- notebook/ : Contient les notebooks d‚Äôexploration, de test ou de d√©monstration.

- tests/ : Scripts de test ou validation du mod√®le/API.

- htmlcov/ : Dossier g√©n√©r√© automatiquement pour les rapports de couverture de test (pytest-cov).

- .github/ : Fichiers li√©s √† la configuration GitHub Actions ou CI/CD.

üìÑ Fichiers racine

- train.py : Script principal pour l‚Äôentra√Ænement du mod√®le.

- inference.py : Script pour ex√©cuter l‚Äôinf√©rence sur un dataset complet.

- app_gradio.py (non visible ici mais probablement dans api/) : Point d'entr√©e de l'interface utilisateur avec Gradio.

- profiling.py : Script de profiling de la fonction predict_credit_score pour d√©tecter les goulots d‚Äô√©tranglement.

- profiling_output.prof, profiling_output3.prof : R√©sultats du profiling √† visualiser avec SnakeViz.

- analyse_logs.py : Analyse et agr√©gation des logs (logs/predictions.log).

- dashboard.py : Script pour cr√©er une visualisation ou un dashboard √† partir des donn√©es de logs.

- requirements.txt : Liste des d√©pendances Python √† installer.

- Dockerfile : Conteneurisation du projet pour un d√©ploiement facile.

- README.md : Documentation principale du projet (actuellement vide).

- .gitignore, .gitattributes : Fichiers de configuration Git.

üîç Fichiers de couverture & cache

- .coverage, .htmlcov/ : G√©n√©r√©s par pytest pour suivre la couverture des tests.

__pycache__/ : Fichiers compil√©s automatiquement par Python (√† ignorer via .gitignore).



#  √âtape 2 ‚Äì D√©ploiement du mod√®le via une API Gradio + Docker + CI/CD


üéØ Objectif

Cette √©tape vise √† exposer le mod√®le de machine learning entra√Æn√© √† l'√©tape 1 via une API accessible. L‚Äôobjectif est triple :

Cr√©er une interface utilisateur (UI) √† l‚Äôaide de Gradio pour permettre √† un utilisateur de faire des pr√©dictions interactives.

Conteneuriser cette application avec Docker pour garantir un d√©ploiement portable et reproductible.

Automatiser le d√©ploiement via une pipeline CI/CD GitHub Actions qui build, teste et d√©ploie automatiquement l‚Äôapplication d√®s qu‚Äôune modification est pouss√©e.

‚öôÔ∏è Fonctionnement g√©n√©ral

1. Interface utilisateur (API Gradio)

Le fichier app_gradio.py contient la fonction principale predict_credit_score() qui :

Re√ßoit des entr√©es utilisateur (11 features s√©lectionn√©es)

Reconstruit un vecteur complet de features pour le mod√®le (full_input)

Applique model.predict_proba() pour g√©n√©rer un score

Traduit ce score en pr√©diction binaire selon un seuil

Retourne le r√©sultat dans une interface simple via Gradio

‚úÖ L‚Äôinterface Gradio s‚Äôouvre dans un navigateur √† l‚Äôadresse : http://localhost:7860

2. Conteneurisation avec Docker

Le Dockerfile permet de :

Cr√©er une image contenant Python, les d√©pendances (requirements.txt) et les scripts

Lancer automatiquement l‚ÄôAPI Gradio au d√©marrage du conteneur

Exemple de build et lancement local :
# Build de l'image
docker build -t credit-risk-api .

# Lancement du conteneur
docker run -p 7860:7860 credit-risk-api

3. Automatisation via CI/CD

Le dossier .github/workflows/ contient le fichier YAML de configuration pour GitHub Actions. Ce pipeline CI/CD :

- Se d√©clenche automatiquement √† chaque push ou pull request sur la branche principale

- Installe les d√©pendances

- Ex√©cute les tests unitaires (s‚Äôil y en a dans le dossier tests/)

- Build l‚Äôimage Docker et d√©ployer sur Hugging Face Spaces 

# pip install -r requirements.txt

# python app_gradio.py

üìÅ Fichiers importants

Fichier / Dossier	R√¥le
app_gradio.py	L‚ÄôAPI Gradio exposant le mod√®le
models/best_model.pkl	Mod√®le XGBoost sauvegard√©
models/features.txt	Liste ordonn√©e des colonnes attendues par le mod√®le
models/threshold.json	Seuil de classification binaire
Dockerfile	Instructions pour cr√©er l‚Äôimage Docker
.github/workflows/	Contient la configuration CI/CD
logs/	Stocke les pr√©dictions faites via l‚ÄôAPI

üß™ Logs & Monitoring

La fonction log_prediction() enregistre chaque appel √† l‚ÄôAPI dans le fichier predictions_log.jsonl avec :

- Entr√©es utilisateur

- Score et pr√©diction

- Temps d‚Äôex√©cution

- Utilisation CPU et RAM

Ces logs peuvent ensuite √™tre analys√©s via analyse_logs.py ou visualis√©s dans un dashboard.py.

‚úÖ Tests r√©alis√©s

Pour garantir le bon fonctionnement, la stabilit√© et les performances de l‚ÄôAPI, plusieurs types de tests ont √©t√© mis en place tout au long de l‚Äô√©tape 2 :

üß™ 1. Tests fonctionnels

Ces tests permettent de s'assurer que l'API Gradio retourne une pr√©diction correcte selon les entr√©es fournies par l'utilisateur :

‚úîÔ∏è V√©rification que la fonction predict_credit_score() retourne une pr√©diction de score et le bon libell√© associ√© (‚úÖ Faible risque ou ‚ùå Risque √©lev√©).

‚úîÔ∏è Tests avec des cas limites (par exemple : score proche du seuil, valeurs nulles ou extr√™mes).

‚úîÔ∏è Contr√¥le que les cases √† cocher (booleans) sont bien converties en 0 ou 1 pour correspondre aux features attendues par le mod√®le.

üß† 2. Tests techniques / unitaires

üîé Test de bon chargement des artefacts :

features.txt contient toutes les colonnes n√©cessaires

Le model.pkl est bien charg√© via joblib

Le threshold.json est correctement lu

üîé Test de compl√©tion automatique des features :
V√©rifie que toutes les colonnes du mod√®le sont bien renseign√©es dans l‚Äôappel predict(), m√™me si l'utilisateur ne fournit que les 10 expos√©es (les autres sont remplies par des 0).

üîé Test de logging :

Chaque pr√©diction doit g√©n√©rer une ligne JSON valide dans le fichier logs/predictions.log.

‚öôÔ∏è 3. Tests de performance

Pour optimiser l'inf√©rence et d√©tecter des goulots d‚Äô√©tranglement :

‚è±Ô∏è Profiling de la fonction predict_credit_score() √† l‚Äôaide de cProfile + snakeviz.

Permet d‚Äôidentifier les fonctions les plus lentes (ex : to_numpy, isna)

‚úÖ Suite √† ces tests, la construction du dictionnaire d‚Äôentr√©e a √©t√© optimis√©e pour gagner plusieurs millisecondes par appel.

üìâ Mesures de :

Temps d‚Äôex√©cution

% CPU utilis√©

M√©moire RAM consomm√©e
Ces donn√©es sont logg√©es automatiquement dans chaque appel.

üîÅ 4. Tests d'int√©gration (API + Docker)

üê≥ V√©rification que l‚ÄôAPI Gradio fonctionne bien dans le conteneur Docker :

Le serveur d√©marre sans erreur (python app_gradio.py)

L'API retourne les m√™mes r√©sultats qu'en local

Les fichiers mod√®les et logs sont bien mont√©s et accessibles

‚öôÔ∏è Tests sur la CI/CD :

√Ä chaque push vers repo distant github, la pipeline GitHub Actions installe les d√©pendances, lance des tests (ou checks de syntaxe / formatage) et peut effectuer un build Docker avant deploiement sur hugging face.

Cela permet de d√©tecter rapidement les r√©gressions ou erreurs d'importation.

- => Rapport de couverture des tests dans le dossier /htmlcov


üîç √âtape 3 ‚Äì Monitoring et d√©tection d‚Äôanomalies
üéØ Objectif

Dans cette troisi√®me √©tape, l'objectif est de surveiller automatiquement l'activit√© de l‚ÄôAPI d√©ploy√©e afin de :

Stocker et centraliser les donn√©es de pr√©diction (inputs, outputs, temps de r√©ponse, ressources syst√®me, etc.).

Analyser ces donn√©es pour d√©tecter des anomalies telles que :

Une d√©rive des donn√©es (data drift)

Une augmentation du temps de r√©ponse (latence)

Une utilisation anormale du CPU ou de la RAM

Une hausse du taux d‚Äôerreur ou des scores inhabituels

üì¶ 1. Collecte et stockage des logs
‚úÖ Donn√©es collect√©es

√Ä chaque appel de l‚ÄôAPI, un log JSON est g√©n√©r√©, contenant :

Cl√©	Description
timestamp	Date et heure UTC de la requ√™te
input	Donn√©es saisies par l'utilisateur
prediction	Score + √©tiquette de risque
duration	Temps de r√©ponse (en secondes)
cpu_percent	Utilisation CPU au moment de la pr√©diction
memory_usage_MB	M√©moire RAM utilis√©e (en m√©gaoctets)

Tous les logs sont stock√©s dans un fichier local : logs/predictions.log


üßº 2. Analyse des logs

Un script Python analyse_logs.py a √©t√© d√©velopp√© pour :

üîé Lire et parser les logs :

Chargement des lignes JSON du fichier logs/predictions.log

Conversion en DataFrame pour une analyse plus simple

üìä Calculer les statistiques cl√©s :

Temps de r√©ponse moyen / max / min

Distribution des scores de risque

Taux de "risques √©lev√©s"

Moyennes mobiles (latence, CPU‚Ä¶)

üìâ D√©tection automatique :

D√©rive des donn√©es : d√©tection d‚Äôun changement significatif dans la distribution des inputs

Exemple : chute ou pic soudain sur EXT_SOURCE_1 ou CRECARD_CNT_DRAWINGS_ATM_CURRENT_mean

Latence anormale : alerte si un appel d√©passe un seuil critique (ex. : 0.5s)

Surconsommation CPU / RAM : si cpu_percent ou memory_usage_MB d√©passe un seuil anormal

üìà 3. Visualisation 


Un dashboard dashboard.py avec Streamlit  :

Afficher les  temps de r√©ponse, CPU, m√©moire

Suivre en temps r√©el la distribution des scores

D√©tecter visuellement les anomalies


√âtape 4 ‚Äì Optimisation des performances du mod√®le en production
Objectif

Maintenant que notre mod√®le est d√©ploy√© et que nous collectons des donn√©es de monitoring, cette √©tape vise √† analyser ses performances en conditions r√©elles, identifier les √©ventuels goulots d‚Äô√©tranglement, tester des strat√©gies d‚Äôoptimisation, et int√©grer les am√©liorations dans le pipeline CI/CD.

1. Analyse des performances en production

Les appels √† l‚ÄôAPI Gradio g√©n√®rent des logs automatiquement via la fonction log_prediction() d√©finie dans le fichier api/logger.py. Chaque appel enregistre les √©l√©ments suivants : timestamp, input utilisateur, score, pr√©diction binaire, temps d‚Äôinf√©rence, pourcentage d'utilisation CPU (cpu_percent), et consommation m√©moire (memory_usage_MB).

Un exemple de log g√©n√©r√© dans logs/predictions.log :

{"timestamp": "2025-09-08T08:50:30.359718+00:00", "input": {"EXT_SOURCE_2": 0.29, "EXT_SOURCE_3": 0.45, ...}, "prediction": "Score : 0.2159 ‚Üí ‚úÖ Faible risque", "duration": 0.0046, "cpu_percent": 0.0, "memory_usage_MB": 260.83}

2. Profiling de l‚ÄôAPI

Un profiling de la fonction predict_credit_score() expos√©e dans app_gradio.py a √©t√© r√©alis√© √† l‚Äôaide du module cProfile. Le fichier profile_api.py contient ce profilage.

Exemple de lancement depuis le terminal :

python profile_api.py

Les r√©sultats sont sauvegard√©s au format .prof et visualis√©s via Snakeviz :

snakeviz logs/profile_predict.prof

Le premier profilage a r√©v√©l√© que la ligne suivante √©tait un goulot d‚Äô√©tranglement :

full_input = {col: 0 for col in all_features}

Cette ligne r√©initialisait un dictionnaire de plus de 300 colonnes √† chaque appel, ce qui cr√©ait une charge inutile.

3. Optimisations mises en ≈ìuvre
a) Optimisation de code dans Gradio

L‚Äôoptimisation principale a consist√© √† cr√©er une version pr√©remplie du dictionnaire full_input une seule fois au chargement de l‚ÄôAPI. Lors de chaque pr√©diction, on fait simplement une copie de ce dictionnaire :

input_template = {col: 0 for col in all_features}

Puis dans la fonction predict_credit_score :

full_input = input_template.copy()
full_input.update(input_dict)

Gr√¢ce √† cette optimisation, le temps d‚Äôinf√©rence est pass√© d‚Äôenviron 0.045 secondes √† 0.004 secondes.

b) Conversion ONNX

Nous avons converti le mod√®le XGBoost .pkl en ONNX avec la biblioth√®que onnxmltools, dans le fichier convert_to_onnx.py. Le mod√®le est ensuite test√© avec onnxruntime.InferenceSession pour l'inf√©rence. Le code de chargement ressemble √† :

session = onnxruntime.InferenceSession("models/best_model.onnx", providers=["CPUExecutionProvider"])

Puis pour l'inf√©rence :

inputs = {session.get_inputs()[0].name: X.astype(np.float32).values}
probas = session.run(None, inputs)[0].ravel()

Cela est visible √† la fin de mon notebook principal : dupli.ipynb dans le dossier notebook


4. Int√©gration dans le pipeline CI/CD

L‚ÄôAPI optimis√©e (app_gradio.py) a √©t√© committ√©e dans le d√©p√¥t GitHub. Le pipeline CI/CD d√©fini dans .github/workflows/deploy.yml permet de :

Ex√©cuter les tests,

Construire l‚Äôimage Docker √† jour,

D√©ployer automatiquement l‚ÄôAPI.

Chaque mise √† jour du code d√©clenche une reconstruction compl√®te.

5. R√©sultats des optimisations

Avant optimisation, l‚Äôinf√©rence sur un appel API prenait environ 0.045 secondes.

Apr√®s optimisation de code (copie du dictionnaire input_template), la dur√©e est descendue √† 0.004 secondes.

Le mod√®le ONNX quant √† lui mettait 0.119 secondes pour inf√©rer le m√™me batch, confirmant que l‚Äôutilisation directe du mod√®le XGBoost .pkl est plus rapide dans notre cas.

Fichiers cl√©s

app_gradio.py : expose l‚ÄôAPI optimis√©e avec Gradio

api/logger.py : fonction log_prediction() qui trace les entr√©es, sorties et m√©triques

logs/predictions.log : fichier de logs des appels API

profile_api.py : script de profiling avec cProfile

convert_to_onnx.py : conversion du mod√®le vers ONNX

quantize_onnx.py : tentative de quantification

.github/workflows/deploy.yml : pipeline CI/CD complet


# Commandes terminales utiles

# üß† Credit Scoring API ‚Äì Projet MLOps

Ce projet a pour objectif de d√©velopper une API de scoring cr√©dit, bas√©e sur un mod√®le XGBoost, avec les √©tapes cl√©s suivantes : entra√Ænement, d√©ploiement, monitoring, optimisation, CI/CD.

---

## üì¶ Commandes terminal utilis√©es dans le projet

### ‚öôÔ∏è ENVIRONNEMENT & D√âPENDANCES

Installation des d√©pendances :
pip install -r requirements.txt


### üß† ENTRA√éNEMENT DU MOD√àLE

Lancer le script d'entra√Ænement :
python train.py

---

### üîç INF√âRENCE EN LOCAL

Lancer l‚Äôinf√©rence sur un jeu de test :
python inference.py

---

### üß™ TESTS UNITAIRES & COUVERTURE

Ex√©cuter les tests avec coverage :
pytest --cov=api tests/

G√©n√©rer un rapport HTML :
pytest --cov=api --cov-report=html


### üìà PROFILING & GOULOTS D'√âTRANGLEMENT

Lancer le profiling :
python profiling.py

Visualiser les r√©sultats :
snakeviz profiling_output3.prof

---

### üñºÔ∏è INTERFACE GRADIO

Lancer l‚Äôinterface Gradio :
python app_gradio.py

Accessible sur : http://localhost:7860

---

### üê≥ DOCKER

Build de l‚Äôimage Docker :
docker build -t credit-risk-api .

Ex√©cuter l‚Äôimage Docker :
docker run -p 7860:7860 credit-risk-api

---

### ü§ñ CI/CD ‚Äì GITHUB ACTIONS

D√©clench√© automatiquement √† chaque push.
Fichier : .github/workflows/deploy.yml

---

### üìä MONITORING & ANALYSE DES LOGS

Analyser les logs :
python analyse_logs.py

Lancer le dashboard (optionnel) :
streamlit run dashboard.py

---



## ‚úÖ Fichiers li√©s

- train.py : Entra√Ænement du mod√®le
- inference.py : Inf√©rence sur donn√©es
- app_gradio.py : API Gradio
- profiling.py : Profiling de l‚Äôinf√©rence
- analyse_logs.py : Analyse automatique des logs
- convert_to_onnx.py : Conversion du mod√®le
- dashboard.py : Dashboard de monitoring
- Dockerfile : Conteneurisation
- requirements.txt : D√©pendances
- .github/workflows/ : Pipeline CI/CD
